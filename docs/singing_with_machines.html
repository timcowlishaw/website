<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="date" content="" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Tim Cowlishaw — Singing with Machines</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Cormorant:ital,wght@0,400;0,600;1,400&family=Urbanist:ital,wght@0,200;0,500;0,900;1,200;1,500;1,900&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="/assets/css/main.css" />
  </head>
  <body>
    <header>
      <h1>Tim Cowlishaw</h1>
      <h2>Designer, Researcher, Technologist</h2>
      <nav>
        <a href="/">Home</a>
        <a href="/creative.html">Creative projects</a>
        <a href="/commercial.html">Commercial work</a>
        <a href="/publications.html">Publications / research activities</a>
        <a href="/cv_en.html">CV [en]</a>
        <a href="/cv_es.html">CV [es]</a>
        <a href="/cv_cat.html">CV [cat]</a>
      </nav>
    </header>
    <main><h3 id="singing-with-machines-2018">Singing with Machines
(2018)</h3>
<h4 id="project-with-bbc-rd">Project with BBC R&amp;D</h4>
<figure>
<img src="/assets/img/singing_with_machines.jpg"
alt="People sat around a table working on laptops, playing with smart speakers and generally looking busy and creative (Photo by Ric Leeson, © BBC 2018)" />
<figcaption aria-hidden="true">People sat around a table working on
laptops, playing with smart speakers and generally looking busy and
creative (Photo by Ric Leeson, © BBC 2018)</figcaption>
</figure>
<p>At the tail end of 2018, <a href="https://prehensile.tumblr.com/"
target="_blank">Henry</a> and I were having an idle conversation about
about smart speakers and generative sound. Henry had been working on the
BBC’s first experiments at producing a skill for the Amazon Alexa, and
we’d been talking for ages about how the rather transactional pattern of
interaction that Alexa affords is a bit underwhelming. As people with an
interest in sound and computational art, we were pretty excited about
the idea of a fairly ubiquitous, programmable, connected speaker, and
the ways that it could be used creatively, so we decided to investigate
how far the device could be pushed in the service of sonic
experimentation.</p>
<p>Henry, seizing this inspiration, immediately went away and converted
<a href="http://inbflat.net/" target="_blank">“In B Flat”</a> by Darren
Solomon into an Alexa skill, and we (assisted by our colleague Lei He)
assembled eight Amazon Echoes for a performance:</p>
<iframe src="https://www.youtube.com/embed/zymfLZzLPXc?si=nLz4lMjOPuWWmKJv" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
</iframe>
<p>This generated a surprising amount of interest online, so we
continued this line of enquiry with added enthusiasm! I became
especially interested in how creative applications of new technologies
could be used critically: to expose and illuminate the hidden workings
of these technologies and their entanglements with society more
generally. Around this time I’d revisited Alvin Lucier’s piece <a
href="https://www.youtube.com/watch?v=fAxHlLK3Oyk" target="_blank">“I am
sitting in a room”</a> – which uses feedback and repeated overdubbing to
slowly transform a spoken word piece into a resonant drone – revealing
the material properties of the tape and acoustic environment through
repetition and feedback. I wondered if the same approach could be used
to better understand the ‘resonances’ or biases of a voice assistant, by
repeatedly applying its speech synthesis and speech recognition in turn,
and observing how the text deteriorates at each step (Update in 2023:
twitter link for the results of this now lost to time since I deleted my
account).</p>
<p>The results were interesting, and Henry then riffed further on this,
producing a pair of Alexa skills allowing two echoes to carry out a
dialogue which slowly ‘disintegrates’. He christened this <a
href="https://www.youtube.com/watch?v=o0n0Dfk1WOU&amp;t=35s"
target="_blank">“I am Running in the Cloud”</a>, and it, again piqued
the curiosity of quite a few people, <a
href="https://languagelog.ldc.upenn.edu/nll/?p=36863"
target="_blank">including the folks at Language Log</a>, which I found
particularly pleasing (see <a
href="http://prehensile.github.io/blog/2018/02/17/running-in-the-cloud.html"
target="_blank">Henry’s write-up of this</a> for more).</p>
<p>One thing we realised over the course of these investigations, is
that these sorts of playful experiments often have huge practical value,
allowing us to discover the ‘edges’ of a technologies capabilities more
easily then when we just use them for the sorts of things for which they
were designed.</p>
<p>We were interested in whether this technique could be extended, and
in particular, whether working with creative people who were unfamiliar
with the technology itself could aid in this process. To investigate, we
enlisted sound artists and musicians <a
href="https://grahamdunning.com/" target="_blank">Graham Dunning</a>, <a
href="https://www.cafeoto.co.uk/artists/natalie-sharp-lone-taxidermist/"
target="_blank">Natalie Sharp</a>, <a href="https://www.liamice.com/"
target="_blank">Lia Mice</a> and <a href="http://www.wesleygoatley.com/"
target="_blank">Wesley Goatley</a> to spend a day with us devising new
works using smart speaker technology, before giving a public performance
of what we’d produced:</p>
<iframe src="https://www.bbc.co.uk/programmes/p06cr1h5/player" frameborder="0">
</iframe>
<p>The results were a great success, leading to ideas we’d never have
thought of ourselves, as well as illuminating the affordances and
limitations of the technology in ways we wouldn’t necessarily have
discovered through our day-to-day work – and the principle of “working
with artists to explore the affordances of new technologies” became
central to our <a href="https://www.youtube.com/watch?v=nl-77GQ9Z68"
target="_blank">unofficial IRFS manifesto</a>. Since then, <a
href="https://fiala.uk/" target="_blank">Jakub</a> and I travelled to
Sonar+D in Barcelona to talk about our work and to <a
href="https://www.bbc.co.uk/rd/blog/2018-06-singing-with-machines"
target="_blank">give a workshop</a> for musicians and artists on design
and prototyping with smart speakers. We also continued our
investigations of musical uses of conversational interfaces –
supervising PhD students <a href="https://lwlsn.github.io/"
target="_blank">Lizzie Wilson</a> and <a href="http://www.delbosque.co/"
target="_blank">Jorge del Bosque</a> on an internship in which they
developed a speech-controlled algorithmic music system called <em><a
href="https://ars.electronica.art/error/en/unspoken/"
target="_blank">Unspoken Word</a></em>, which was exhibited at Ars
Electronica 2018.</p></main>
  </body>
</html>
